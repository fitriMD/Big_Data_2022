{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0Q42J3rxC3Hw/eLBgqG6U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fitriMD/Big_Data_2022/blob/main/PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark ditulis menggunakan bahasa pemrogrraman Scala dan membutuhkan Java Virtual Machine (JVM) untuk menjalankannya. Sebagai langkah pertama, lakukan instalasi Java dengan menuliskan perintah di bawah ini."
      ],
      "metadata": {
        "id": "DHO41gXEmp8_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TbBy-76ylxiu"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Untuk instalasi Apache Spark, unduh berkas menggunakan perintah wget kemudian ekstrak dengan perintah tar. Silahkan salin perintah berikut untuk melakukan instalasi."
      ],
      "metadata": {
        "id": "UpdAIYdGm1QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "EUrmXE1Omehw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sebagai Langkah lanjutan, dibutuhkan pengaturan terkait Java dan Spark Home. Untuk melakukannya dapat memanfaatkan script pyton. Silahkan masukkan kode berikut ke dalam notebook."
      ],
      "metadata": {
        "id": "hAmvH3Qsm76l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "9aItb9Jsm0TJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Konfigurasi PySpark dapat dilakukan dengan menginstall library findspark yang digunakan untuk mencari lokasi Spark yang terinstall pada sistem. Proses instalasi dapat memanfaatkan perintah pip, silahkan perhatikan perintah di bawah ini."
      ],
      "metadata": {
        "id": "fgF3fOJunEzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "PrJqSNzCnJGn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setelah proses instalasi berhasil, lakukan import library dan inisialisasi findspark. Silahkan salin kode berikut ke dalam notebook."
      ],
      "metadata": {
        "id": "Ef7x4R81nMbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "MS0WImEDnREw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Koneksi ke dalam spark dapat dilakukan memanfaatkan SparkSession. Salin kode berikut, dimana spark menggunakan port 4050."
      ],
      "metadata": {
        "id": "5MOr5GcwnT8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        ".master(\"local\")\\\n",
        ".appName(\"Colab\")\\\n",
        ".config('spark.ui.port', '4050')\\\n",
        ".getOrCreate()"
      ],
      "metadata": {
        "id": "tqOynf8VnXWO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark mempunyai beberapa modul untuk membaca data dengan format yang berbeda. Spark secara otomatis akan menentukan tiap tipe data untuk setiap kolom. Data yang akan digunakan sebagai dataset dapat diunduh menggunakan perintah wget. Silahkan perhatikan perintah berikut."
      ],
      "metadata": {
        "id": "QT202mp6ncP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --continue https://github.com/dhanifudin/pyspark-demo -O sample_books.json"
      ],
      "metadata": {
        "id": "gA381V7pnhL1",
        "outputId": "402c55a8-3ae1-48d4-a85f-0805616f951f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-06 06:16:28--  https://github.com/dhanifudin/pyspark-demo\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘sample_books.json’\n",
            "\n",
            "sample_books.json       [ <=>                ] 143.14K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-06-06 06:16:28 (9.73 MB/s) - ‘sample_books.json’ saved [146580]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data yang telah diunduh dapat dibaca dengan menggunakan kode berikut. Silahkan salin kode berikut ke dalam notebook."
      ],
      "metadata": {
        "id": "kKARgrYintsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"sample_books.json\")"
      ],
      "metadata": {
        "id": "ARNJtl8lnxsP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sebelum dapat menganalisa dataset, perlu mengetahui schema data yang akan diolah. Schema dapat diketahui dengan menggunakan kode berikut. Kode ini memanfaatkan dataframe."
      ],
      "metadata": {
        "id": "whhRPxZnn3pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "Mj7ky97Rn8EE",
        "outputId": "d864fdcf-7901-44cf-fb58-5f100f9da36d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            " |-- author: string (nullable = true)\n",
            " |-- edition: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- year_written: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(4,False)"
      ],
      "metadata": {
        "id": "Hl9aJYpVohEN",
        "outputId": "fd1a586f-223f-4e6f-a014-3cc2d0c9790f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+--------------+-----+----------------+------------+\n",
            "|_corrupt_record|author         |edition       |price|title           |year_written|\n",
            "+---------------+---------------+--------------+-----+----------------+------------+\n",
            "|null           |Austen, Jane   |Penguin       |18.2 |Northanger Abbey|1814        |\n",
            "|null           |Tolstoy, Leo   |Penguin       |12.7 |War and Peace   |1865        |\n",
            "|null           |Tolstoy, Leo   |Penguin       |13.5 |Anna Karenina   |1875        |\n",
            "|null           |Woolf, Virginia|Harcourt Brace|25.0 |Mrs. Dalloway   |1925        |\n",
            "+---------------+---------------+--------------+-----+----------------+------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "K7ir861uqNbG",
        "outputId": "60f83f58-5510-486b-a2f6-7946b61315ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1267"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"title\", \"price\", \"year_written\").show(5)"
      ],
      "metadata": {
        "id": "VAPqDJICqbV5",
        "outputId": "f4a2c6a7-12f9-47f9-e87c-0f1f1d1af56c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----+------------+\n",
            "|           title|price|year_written|\n",
            "+----------------+-----+------------+\n",
            "|Northanger Abbey| 18.2|        1814|\n",
            "|   War and Peace| 12.7|        1865|\n",
            "|   Anna Karenina| 13.5|        1875|\n",
            "|   Mrs. Dalloway| 25.0|        1925|\n",
            "|       The Hours|12.35|        1999|\n",
            "+----------------+-----+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get books that are written after 1950 & cost greater than $10\n",
        "\n",
        "df_filtered = df.filter(\"year_written > 1950 AND price > 10 AND title IS NOT NULL\")\n",
        "df_filtered.select(\"title\", \"price\", \"year_written\").show(50, False)"
      ],
      "metadata": {
        "id": "bM0YdHGzqd6p",
        "outputId": "ada489fa-20fb-4a3b-8ce9-8747b64c6d3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+-----+------------+\n",
            "|title                        |price|year_written|\n",
            "+-----------------------------+-----+------------+\n",
            "|The Hours                    |12.35|1999        |\n",
            "|Harry Potter                 |19.95|2000        |\n",
            "|One Hundred Years of Solitude|14.0 |1967        |\n",
            "+-----------------------------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "\n",
        "# Find the costliest book\n",
        "maxValue = df_filtered.agg(max(\"price\")).collect()[0][0]\n",
        "print(\"maxValue: \",maxValue)\n",
        "\n",
        "df_filtered.select(\"title\",\"price\").filter(df.price == maxValue).show(20, False)"
      ],
      "metadata": {
        "id": "FSWQnA5LqgJh",
        "outputId": "850a5998-3966-4105-c2a7-7ac0c9751fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maxValue:  19.95\n",
            "+------------+-----+\n",
            "|title       |price|\n",
            "+------------+-----+\n",
            "|Harry Potter|19.95|\n",
            "+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TUGAS**"
      ],
      "metadata": {
        "id": "l-DYmKJpqjqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1\n",
        "from pyspark.sql.functions import min\n",
        "\n",
        "# Find the cheapest price\n",
        "\n",
        "minValue = df.agg(min(\"price\")).collect()[0][0]\n",
        "print(\"minValue: \",minValue)\n",
        "\n",
        "df.select(\"author\", \"edition\", \"price\", \"title\", \"year_written\").filter(df.price == minValue).show(20, False)"
      ],
      "metadata": {
        "id": "PAMPlau7qlKW",
        "outputId": "fe18221e-59cf-4ad1-81c7-003f2a0801a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minValue:  5.75\n",
            "+----------------+------------+-----+-----------+------------+\n",
            "|author          |edition     |price|title      |year_written|\n",
            "+----------------+------------+-----+-----------+------------+\n",
            "|Dickens, Charles|Random House|5.75 |Bleak House|1870        |\n",
            "+----------------+------------+-----+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "df.groupBy(\"year_written\").count().select(\"year_written\", f.col(\"count\").\n",
        "                                      alias(\"jumlah_buku\")).sort(desc(\"year_written\")).show()"
      ],
      "metadata": {
        "id": "07C35DdYqwS2",
        "outputId": "fa3ad526-67fe-4b4e-a017-cb2c7837ad4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+\n",
            "|year_written|jumlah_buku|\n",
            "+------------+-----------+\n",
            "|        2000|          1|\n",
            "|        1999|          1|\n",
            "|        1967|          1|\n",
            "|        1937|          1|\n",
            "|        1925|          1|\n",
            "|        1922|          1|\n",
            "|        1875|          1|\n",
            "|        1870|          1|\n",
            "|        1865|          2|\n",
            "|        1862|          1|\n",
            "|        1814|          1|\n",
            "|        1603|          1|\n",
            "|        null|       1254|\n",
            "+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "df.groupBy(\"year_written\").agg({\"Price\" : \"max\"}).sort(desc(\"year_written\")).show()\n"
      ],
      "metadata": {
        "id": "fkEXoQtRq6Vl",
        "outputId": "f7f166cb-61dd-4ffc-d88b-b5f7a390aee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+\n",
            "|year_written|max(Price)|\n",
            "+------------+----------+\n",
            "|        2000|     19.95|\n",
            "|        1999|     12.35|\n",
            "|        1967|      14.0|\n",
            "|        1937|     27.45|\n",
            "|        1925|      25.0|\n",
            "|        1922|      29.0|\n",
            "|        1875|      13.5|\n",
            "|        1870|      5.75|\n",
            "|        1865|      12.7|\n",
            "|        1862|      7.75|\n",
            "|        1814|      18.2|\n",
            "|        1603|      7.95|\n",
            "|        null|      null|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "df.groupBy(\"year_written\").agg({\"Price\" : \"min\"}).sort(desc(\"year_written\")).show()"
      ],
      "metadata": {
        "id": "n6tw7T5yrCiJ",
        "outputId": "0bbd4578-06c6-4ed3-a1d5-52c17b3dfd2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+\n",
            "|year_written|min(Price)|\n",
            "+------------+----------+\n",
            "|        2000|     19.95|\n",
            "|        1999|     12.35|\n",
            "|        1967|      14.0|\n",
            "|        1937|     27.45|\n",
            "|        1925|      25.0|\n",
            "|        1922|      29.0|\n",
            "|        1875|      13.5|\n",
            "|        1870|      5.75|\n",
            "|        1865|      5.76|\n",
            "|        1862|      7.75|\n",
            "|        1814|      18.2|\n",
            "|        1603|      7.95|\n",
            "|        null|      null|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}